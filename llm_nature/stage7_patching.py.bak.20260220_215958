import gc
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from .util import RunContext, banner, write_json, write_text

def _topk(tokenizer, probs: torch.Tensor, k: int = 10) -> List[Dict[str, Any]]:
    vals, idx = torch.topk(probs, k)
    out = []
    for p, tid in zip(vals.tolist(), idx.tolist()):
        tok = tokenizer.decode([tid])
        out.append({"token": tok, "token_id": int(tid), "p": float(p)})
    return out

@dataclass
class _Capture:
    handle: Any = None
    value: torch.Tensor = None

def _load(model_name: str, device: str):
    gc.collect()
    if device.startswith("cuda") and torch.cuda.is_available():
        torch.cuda.empty_cache()
    tok = AutoTokenizer.from_pretrained(model_name)
    mdl = AutoModelForCausalLM.from_pretrained(model_name).to(device)
    mdl.eval()
    return mdl, tok

def _pick_device() -> str:
    if torch.cuda.is_available():
        return "cuda"
    return "cpu"

def run(ctx: RunContext) -> Dict[str, Any]:
    stage = 7
    model_name = "gpt2"
    device = _pick_device()

    clean = "The capital of France is"
    corrupted = "The capital of England is"

    layer = 6

    lines: List[str] = []
    lines.append(banner("EXPERIMENT 7: ACTIVATION PATCHING â€” GPT-2 BRAIN SURGERY"))
    lines.append("ðŸ§‘â€ðŸ¤â€ðŸ§‘ Friend-finishes-your-sentence check (Stage 7)")
    lines.append(f"   You:      {clean} ...")
    lines.append("   Friend:   London. (confidently, but wrong)")
    lines.append("   Reality:  We can patch an internal vector and watch the guess shift.\n")
    lines.append(f"Device: {device}")
    lines.append(f"Model: {model_name}")
    lines.append("Patch site: layer=6, position=last_token\n")
    lines.append(f"Corrupted prompt: {corrupted!r}\n")

    mdl, tok = _load(model_name, device)

    def encode(prompt: str) -> torch.Tensor:
        ids = tok(prompt, return_tensors="pt").input_ids.to(device)
        return ids

    ids_clean = encode(clean)
    ids_corr = encode(corrupted)

    pos_clean = int(ids_clean.shape[1] - 1)
    pos_corr = int(ids_corr.shape[1] - 1)

    cap_clean = _Capture()
    cap_corr = _Capture()

    def hook_clean(module, inp, out):
        x = out[0] if isinstance(out, tuple) else out
        cap_clean.value = x.detach()

    def hook_corr(module, inp, out):
        x = out[0] if isinstance(out, tuple) else out
        cap_corr.value = x.detach()

    blk = mdl.transformer.h[layer]
    cap_clean.handle = blk.register_forward_hook(hook_clean)
    with torch.no_grad():
        out_clean = mdl(ids_clean)
    cap_clean.handle.remove()

    cap_corr.handle = blk.register_forward_hook(hook_corr)
    with torch.no_grad():
        out_corr = mdl(ids_corr)
    cap_corr.handle.remove()

    logits_before = out_corr.logits[0, -1, :]
    probs_before = torch.softmax(logits_before, dim=-1)
    top_before = _topk(tok, probs_before, k=10)

    clean_vec = cap_clean.value[:, pos_clean, :].clone()

    def hook_patch(module, inp, out):
        x = out[0] if isinstance(out, tuple) else out
        x = x.clone()
        x[:, pos_corr, :] = clean_vec
        return (x,) + out[1:] if isinstance(out, tuple) else x

    h = blk.register_forward_hook(hook_patch)
    with torch.no_grad():
        out_patched = mdl(ids_corr)
    h.remove()

    logits_after = out_patched.logits[0, -1, :]
    probs_after = torch.softmax(logits_after, dim=-1)
    top_after = _topk(tok, probs_after, k=10)

    lines.append("Top-10 next tokens BEFORE patch:")
    for t in top_before:
        lines.append(f"   {t['token']!r:10} p={t['p']:.4f}")
    lines.append("")
    lines.append("Top-10 next tokens AFTER patch (clean vector injected):")
    for t in top_after:
        lines.append(f"   {t['token']!r:10} p={t['p']:.4f}")
    lines.append("")

    txt = "\n".join(lines)
    write_text(f"{ctx.out_dir}/printed_output.txt", txt)
    write_json(f"{ctx.out_dir}/artifacts.json", {
        "stage": stage,
        "seed": ctx.seed,
        "device": device,
        "model": model_name,
        "clean_prompt": clean,
        "corrupted_prompt": corrupted,
        "patch_site": {"layer": layer, "position_clean": pos_clean, "position_corrupted": pos_corr},
        "top_before": top_before,
        "top_after": top_after,
    })

    return {"stage": stage, "printed": txt}
